{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":15444,"sourceType":"datasetVersion","datasetId":11102},{"sourceId":714968,"sourceType":"datasetVersion","datasetId":366471},{"sourceId":596534,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":446705,"modelId":463171}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 1: Installations\n!pip install -q timm py7zr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T07:34:21.320044Z","iopub.execute_input":"2025-10-03T07:34:21.320703Z","iopub.status.idle":"2025-10-03T07:35:45.269233Z","shell.execute_reply.started":"2025-10-03T07:34:21.320676Z","shell.execute_reply":"2025-10-03T07:35:45.268212Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.3/141.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.9/412.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 2: Imports & Configuration\nimport os\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport py7zr\nfrom tqdm.notebook import tqdm\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchvision.transforms as T\nimport timm\n\n# --- Configuration ---\nclass CFG:\n    # Kaggle environment paths\n    data_path = '/kaggle/input/cifar10-python/cifar-10-batches-py'\n    train_archive = os.path.join(data_path, 'train.npy')\n    test_archive = os.path.join(data_path, 'test.npy')\n    train_labels_csv = os.path.join(data_path, 'trainLabels.csv')\n    \n    # Output paths\n    extract_path = '/kaggle/working/cifar10/'\n    train_img_path = os.path.join(extract_path, 'train')\n    test_img_path = os.path.join(extract_path, 'test')\n    model_save_path = '/kaggle/input/iisc-vit/pytorch/default/1/iisc vit.pth'\n\n    # Model and Training parameters\n    model_name = 'vit_base_patch16_224.augreg_in21k' # A powerful pre-trained ViT\n    img_size = 224\n    batch_size = 32\n    epochs = 50  # A good starting point for fine-tuning\n    learning_rate = 1e-4\n    weight_decay = 1e-5\n    label_smoothing = 0.1\n    validation_split = 0.1 # Use 10% of training data for validation\n    \n    # Hardware\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # CIFAR-10 specific\n    num_classes = 10\n    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n\nprint(f\"Using device: {CFG.device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T07:38:39.020333Z","iopub.execute_input":"2025-10-03T07:38:39.020596Z","iopub.status.idle":"2025-10-03T07:38:39.027379Z","shell.execute_reply.started":"2025-10-03T07:38:39.020577Z","shell.execute_reply":"2025-10-03T07:38:39.026764Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import pickle\nimport numpy as np\n\ndef load_cifar_batch(filename):\n    with open(filename, 'rb') as f:\n        batch = pickle.load(f, encoding='latin1')\n    data = batch['data']\n    labels = batch['labels']\n    data = data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)  # (N, 32, 32, 3)\n    return data, np.array(labels)  # <-- convert to numpy here\n\n# Load all training batches\ntrain_data, train_labels = [], []\nfor i in range(1, 6):\n    data, labels = load_cifar_batch(os.path.join(CFG.data_path, f'data_batch_{i}'))\n    train_data.append(data)\n    train_labels.append(labels)\n\ntrain_data = np.concatenate(train_data)         # (50000, 32, 32, 3)\ntrain_labels = np.concatenate(train_labels)     # (50000,)\n\n# Load test batch\ntest_data, test_labels = load_cifar_batch(os.path.join(CFG.data_path, 'test_batch'))\n\nprint(\"Train data:\", train_data.shape, \"Labels:\", train_labels.shape)\nprint(\"Test data:\", test_data.shape, \"Labels:\", test_labels.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T07:36:14.824933Z","iopub.execute_input":"2025-10-03T07:36:14.825480Z","iopub.status.idle":"2025-10-03T07:36:17.153819Z","shell.execute_reply.started":"2025-10-03T07:36:14.825454Z","shell.execute_reply":"2025-10-03T07:36:17.153060Z"}},"outputs":[{"name":"stdout","text":"Train data: (50000, 32, 32, 3) Labels: (50000,)\nTest data: (10000, 32, 32, 3) Labels: (10000,)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"os.makedirs(CFG.train_img_path, exist_ok=True)\nos.makedirs(CFG.test_img_path, exist_ok=True)\n\n# Save training images\nfor idx, (img_array, label) in tqdm(enumerate(zip(train_data, train_labels)), total=len(train_data)):\n    class_name = CFG.class_names[label]\n    class_dir = os.path.join(CFG.train_img_path, class_name)\n    os.makedirs(class_dir, exist_ok=True)\n    img = Image.fromarray(img_array.astype('uint8'))\n    img.save(os.path.join(class_dir, f\"{idx}.png\"))\n\n# Save test images\nfor idx, (img_array, label) in tqdm(enumerate(zip(test_data, test_labels)), total=len(test_data)):\n    class_name = CFG.class_names[label]\n    class_dir = os.path.join(CFG.test_img_path, class_name)\n    os.makedirs(class_dir, exist_ok=True)\n    img = Image.fromarray(img_array.astype('uint8'))\n    img.save(os.path.join(class_dir, f\"{idx}.png\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T07:36:17.154966Z","iopub.execute_input":"2025-10-03T07:36:17.155243Z","iopub.status.idle":"2025-10-03T07:36:40.532544Z","shell.execute_reply.started":"2025-10-03T07:36:17.155223Z","shell.execute_reply":"2025-10-03T07:36:40.531830Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89b78df5ebab43069f9a1414493b19d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d23ee341f704380a728061c4a624e06"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Expected train samples:\", train_data.shape[0])\nprint(\"Files saved:\", len(os.listdir(CFG.train_img_path)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T07:36:45.939963Z","iopub.execute_input":"2025-10-03T07:36:45.940598Z","iopub.status.idle":"2025-10-03T07:36:45.944984Z","shell.execute_reply.started":"2025-10-03T07:36:45.940573Z","shell.execute_reply":"2025-10-03T07:36:45.944361Z"}},"outputs":[{"name":"stdout","text":"Expected train samples: 50000\nFiles saved: 10\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4: Dataset & DataLoaders\n\nclass Cifar10Dataset(Dataset):\n    def __init__(self, images, labels=None, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_array = self.images[idx]\n        image = Image.fromarray(img_array.astype('uint8'))  # Convert numpy -> PIL\n\n        if self.transform:\n            image = self.transform(image)\n\n        if self.labels is not None:  # training/validation\n            label = self.labels[idx]\n            return image, torch.tensor(label, dtype=torch.long)\n        else:  # test set (labels unknown)\n            return image, idx\n\n# Define data augmentations and normalization\ndata_transforms = {\n    'train': T.Compose([\n        T.Resize((CFG.img_size, CFG.img_size)),\n        T.TrivialAugmentWide(),\n        T.ToTensor(),\n        T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) \n    ]),\n    'val': T.Compose([\n        T.Resize((CFG.img_size, CFG.img_size)),\n        T.ToTensor(),\n        T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    ]),\n}\n\n# --- Create full dataset from numpy arrays ---\nfull_dataset = Cifar10Dataset(train_data, train_labels, transform=None)\n\n# Split into train/validation\nval_size = int(len(full_dataset) * CFG.validation_split)\ntrain_size = len(full_dataset) - val_size\ntrain_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n\n# Apply transforms to each split\ntrain_dataset.dataset.transform = data_transforms['train']\nval_dataset.dataset.transform = data_transforms['val']\n\n# --- Create DataLoaders ---\ntrain_loader = DataLoader(\n    train_dataset, batch_size=CFG.batch_size, shuffle=True,\n    num_workers=os.cpu_count(), pin_memory=True\n)\nval_loader = DataLoader(\n    val_dataset, batch_size=CFG.batch_size, shuffle=False,\n    num_workers=os.cpu_count(), pin_memory=True\n)\n\nprint(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T07:37:08.404989Z","iopub.execute_input":"2025-10-03T07:37:08.405285Z","iopub.status.idle":"2025-10-03T07:37:08.420981Z","shell.execute_reply.started":"2025-10-03T07:37:08.405263Z","shell.execute_reply":"2025-10-03T07:37:08.420371Z"}},"outputs":[{"name":"stdout","text":"Training samples: 45000, Validation samples: 5000\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Cell 5: Model, Loss, Optimizer Setup\n\n# Load pre-trained ViT model\nmodel = timm.create_model(\n    CFG.model_name,\n    pretrained=True,\n    num_classes=CFG.num_classes\n)\nmodel.to(CFG.device)\n\n# Loss Function with Label Smoothing\n# This regularization prevents the model from becoming overconfident\nloss_fn = nn.CrossEntropyLoss(label_smoothing=CFG.label_smoothing)\n\n# AdamW Optimizer - better than Adam for generalization\noptimizer = torch.optim.AdamW(model.parameters(), lr=CFG.learning_rate, weight_decay=CFG.weight_decay)\n\n# OneCycleLR Scheduler for faster convergence and better performance\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=CFG.learning_rate,\n    steps_per_epoch=len(train_loader),\n    epochs=CFG.epochs\n)\n\n# Gradient Scaler for Automatic Mixed Precision (AMP)\n# This speeds up training significantly on modern GPUs\nscaler = torch.cuda.amp.GradScaler()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T07:37:09.219976Z","iopub.execute_input":"2025-10-03T07:37:09.220697Z","iopub.status.idle":"2025-10-03T07:37:13.808564Z","shell.execute_reply.started":"2025-10-03T07:37:09.220663Z","shell.execute_reply":"2025-10-03T07:37:13.807729Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/410M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dd4f366a5ae4d269f02d9c8af71ff0c"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/3930098787.py:28: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Cell 6: Training & Validation Loop\n\ndef train_one_epoch(model, loader, optimizer, scheduler, loss_fn, scaler, device):\n    model.train()\n    total_loss = 0\n    progress_bar = tqdm(loader, desc=\"Training\", leave=False)\n    \n    for inputs, labels in progress_bar:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Mixed precision training context\n        with torch.cuda.amp.autocast():\n            outputs = model(inputs)\n            loss = loss_fn(outputs, labels)\n        \n        # Backward pass with scaler\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        scheduler.step() # Step scheduler at each batch for OneCycleLR\n        \n        total_loss += loss.item()\n        progress_bar.set_postfix(loss=loss.item(), lr=scheduler.get_last_lr()[0])\n        \n    return total_loss / len(loader)\n\ndef validate_one_epoch(model, loader, loss_fn, device):\n    model.eval()\n    total_loss = 0\n    correct_predictions = 0\n    total_samples = 0\n    \n    with torch.no_grad():\n        progress_bar = tqdm(loader, desc=\"Validating\", leave=False)\n        for inputs, labels in progress_bar:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            with torch.cuda.amp.autocast():\n                outputs = model(inputs)\n                loss = loss_fn(outputs, labels)\n\n            total_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            correct_predictions += torch.sum(preds == labels.data)\n            total_samples += labels.size(0)\n\n    avg_loss = total_loss / len(loader)\n    accuracy = correct_predictions.double() / total_samples\n    return avg_loss, accuracy\n\n# --- Main Training Loop ---\nbest_val_accuracy = 0.0\n\nfor epoch in range(CFG.epochs):\n    print(f\"--- Epoch {epoch+1}/{CFG.epochs} ---\")\n    \n    train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, loss_fn, scaler, CFG.device)\n    val_loss, val_accuracy = validate_one_epoch(model, val_loader, loss_fn, CFG.device)\n    \n    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.4f}\")\n    \n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        print(f\"New best model found! Saving to {CFG.model_save_path}\")\n        torch.save(model.state_dict(), CFG.model_save_path)\n\nprint(\"\\n--- Training Finished ---\")\nprint(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T21:44:43.993075Z","iopub.execute_input":"2025-10-02T21:44:43.993308Z","execution_failed":"2025-10-03T01:52:54.555Z"}},"outputs":[{"name":"stdout","text":"--- Epoch 1/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/2259735591.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/2259735591.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss: 0.6092 | Val Loss: 0.5389 | Val Accuracy: 0.9870\nNew best model found! Saving to /kaggle/working/best_model.pth\n--- Epoch 2/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2: Train Loss: 0.5211 | Val Loss: 0.5332 | Val Accuracy: 0.9900\nNew best model found! Saving to /kaggle/working/best_model.pth\n--- Epoch 3/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3: Train Loss: 0.5211 | Val Loss: 0.5525 | Val Accuracy: 0.9814\n--- Epoch 4/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4: Train Loss: 0.5293 | Val Loss: 0.5583 | Val Accuracy: 0.9808\n--- Epoch 5/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5: Train Loss: 0.5323 | Val Loss: 0.5520 | Val Accuracy: 0.9802\n--- Epoch 6/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 6: Train Loss: 0.5391 | Val Loss: 0.6082 | Val Accuracy: 0.9608\n--- Epoch 7/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7: Train Loss: 0.5491 | Val Loss: 0.5972 | Val Accuracy: 0.9618\n--- Epoch 8/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 8: Train Loss: 0.5548 | Val Loss: 0.5984 | Val Accuracy: 0.9604\n--- Epoch 9/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 9: Train Loss: 0.5594 | Val Loss: 0.5990 | Val Accuracy: 0.9574\n--- Epoch 10/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10: Train Loss: 0.5583 | Val Loss: 0.6197 | Val Accuracy: 0.9504\n--- Epoch 11/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 11: Train Loss: 0.5687 | Val Loss: 0.6257 | Val Accuracy: 0.9480\n--- Epoch 12/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 12: Train Loss: 0.5653 | Val Loss: 0.6251 | Val Accuracy: 0.9474\n--- Epoch 13/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 13: Train Loss: 0.5649 | Val Loss: 0.6638 | Val Accuracy: 0.9326\n--- Epoch 14/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 14: Train Loss: 0.5606 | Val Loss: 0.6523 | Val Accuracy: 0.9420\n--- Epoch 15/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 15: Train Loss: 0.5623 | Val Loss: 0.6223 | Val Accuracy: 0.9482\n--- Epoch 16/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 16: Train Loss: 0.5530 | Val Loss: 0.6375 | Val Accuracy: 0.9432\n--- Epoch 17/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90cd14f187e94b748c8e9ef881367603"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63aa75c4f9574c7ab0dec9463c36e90a"}},"metadata":{}},{"name":"stdout","text":"Epoch 17: Train Loss: 0.5479 | Val Loss: 0.6761 | Val Accuracy: 0.9316\n--- Epoch 18/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca3e1cae31804645a66cd43208931fd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0165838392404c6fa050a616a81d5075"}},"metadata":{}},{"name":"stdout","text":"Epoch 18: Train Loss: 0.5469 | Val Loss: 0.6447 | Val Accuracy: 0.9428\n--- Epoch 19/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f5c793c852e4fe19ec923ba23cc8b00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"130c099b1c794688a5a2596993bd69a1"}},"metadata":{}},{"name":"stdout","text":"Epoch 19: Train Loss: 0.5454 | Val Loss: 0.6286 | Val Accuracy: 0.9484\n--- Epoch 20/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"094afafbeaf047d0823343e41ba3e887"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29d184da0332485d99d652efecd11320"}},"metadata":{}},{"name":"stdout","text":"Epoch 20: Train Loss: 0.5384 | Val Loss: 0.6528 | Val Accuracy: 0.9408\n--- Epoch 21/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e20780ab086b45a68e5d109e1a2964cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bd84dc3c15c472da398a89146f2c670"}},"metadata":{}},{"name":"stdout","text":"Epoch 21: Train Loss: 0.5384 | Val Loss: 0.6444 | Val Accuracy: 0.9448\n--- Epoch 22/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2589c218307443ddad29ffed8005e591"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80158efb0f6b44858cf082604e6b6b3e"}},"metadata":{}},{"name":"stdout","text":"Epoch 22: Train Loss: 0.5336 | Val Loss: 0.6252 | Val Accuracy: 0.9518\n--- Epoch 23/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38a2a900d2844e748d618e69c6120be2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69e580f8344c4d36a41d69b7040dcde3"}},"metadata":{}},{"name":"stdout","text":"Epoch 23: Train Loss: 0.5281 | Val Loss: 0.6344 | Val Accuracy: 0.9472\n--- Epoch 24/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42881dc722704b7d94032bca0e15e9f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59db2539c3fd485f9b10c353e404a2df"}},"metadata":{}},{"name":"stdout","text":"Epoch 24: Train Loss: 0.5281 | Val Loss: 0.6568 | Val Accuracy: 0.9396\n--- Epoch 25/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb4a3c69f7ad41bfbb1214d2519acc27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5ab884a192d4c21a6be7a2fe6532845"}},"metadata":{}},{"name":"stdout","text":"Epoch 25: Train Loss: 0.5269 | Val Loss: 0.6418 | Val Accuracy: 0.9438\n--- Epoch 26/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30026f2fc77d43c4988cc1f7ddcc2084"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f31ab5e683c04617b17a808404b804c2"}},"metadata":{}},{"name":"stdout","text":"Epoch 26: Train Loss: 0.5208 | Val Loss: 0.6291 | Val Accuracy: 0.9506\n--- Epoch 27/50 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18ce25e2aa254c78ba1fee8651118248"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nimport numpy as np\nimport pickle\n\n# 1. Reload the test batch\ndef load_cifar_batch(filename):\n    with open(filename, 'rb') as f:\n        batch = pickle.load(f, encoding='latin1')\n    data = batch['data']\n    labels = batch['labels']\n    data = data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)  # (N, 32, 32, 3)\n    return data, np.array(labels)\n\ntest_data, test_labels = load_cifar_batch(\"/kaggle/input/cifar10-python/cifar-10-batches-py/test_batch\")\n\n# 2. Create Dataset class (same as training)\nclass Cifar10Dataset(torch.utils.data.Dataset):\n    def __init__(self, images, labels=None, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_array = self.images[idx]\n        image = Image.fromarray(img_array.astype('uint8'))  # numpy -> PIL\n        if self.transform:\n            image = self.transform(image)\n        if self.labels is not None:\n            label = self.labels[idx]\n            return image, torch.tensor(label, dtype=torch.long)\n        else:\n            return image, idx\n\n# 3. Use the same validation transforms\ntest_transform = T.Compose([\n    T.Resize((CFG.img_size, CFG.img_size)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\ntest_dataset = Cifar10Dataset(test_data, test_labels, transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=os.cpu_count())\n\n# 4. Load the trained model\nmodel = timm.create_model(CFG.model_name, pretrained=False, num_classes=CFG.num_classes)\nmodel.load_state_dict(torch.load(CFG.model_save_path, map_location=CFG.device))\nmodel.to(CFG.device)\nmodel.eval()\n\n# 5. Evaluate accuracy\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(CFG.device), labels.to(CFG.device)\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\ntest_accuracy = correct / total * 100\nprint(f\"✅ Test Accuracy: {test_accuracy:.2f}%\")\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nimport numpy as np\nimport pickle\n\n# 1. Reload the test batch\ndef load_cifar_batch(filename):\n    with open(filename, 'rb') as f:\n        batch = pickle.load(f, encoding='latin1')\n    data = batch['data']\n    labels = batch['labels']\n    data = data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)  # (N, 32, 32, 3)\n    return data, np.array(labels)\n\ntest_data, test_labels = load_cifar_batch(\"/kaggle/input/cifar10-python/cifar-10-batches-py/test_batch\")\n\n# 2. Create Dataset class (same as training)\nclass Cifar10Dataset(torch.utils.data.Dataset):\n    def __init__(self, images, labels=None, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_array = self.images[idx]\n        image = Image.fromarray(img_array.astype('uint8'))  # numpy -> PIL\n        if self.transform:\n            image = self.transform(image)\n        if self.labels is not None:\n            label = self.labels[idx]\n            return image, torch.tensor(label, dtype=torch.long)\n        else:\n            return image, idx\n\n# 3. Use the same validation transforms\ntest_transform = T.Compose([\n    T.Resize((CFG.img_size, CFG.img_size)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\ntest_dataset = Cifar10Dataset(test_data, test_labels, transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=os.cpu_count())\n\n# 4. Load the trained model\nmodel = timm.create_model(CFG.model_name, pretrained=False, num_classes=CFG.num_classes)\nmodel.load_state_dict(torch.load(CFG.model_save_path, map_location=CFG.device))\nmodel.to(CFG.device)\nmodel.eval()\n\n# 5. Evaluate accuracy\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(CFG.device), labels.to(CFG.device)\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\ntest_accuracy = correct / total * 100\nprint(f\"✅ Test Accuracy: {test_accuracy:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T07:38:42.622959Z","iopub.execute_input":"2025-10-03T07:38:42.623518Z","iopub.status.idle":"2025-10-03T07:42:37.042995Z","shell.execute_reply.started":"2025-10-03T07:38:42.623493Z","shell.execute_reply":"2025-10-03T07:42:37.041950Z"}},"outputs":[{"name":"stdout","text":"✅ Test Accuracy: 98.77%\n✅ Test Accuracy: 98.77%\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}